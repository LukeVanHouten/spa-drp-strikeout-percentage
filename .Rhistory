sub_sample = sub_samples))
best_mae <- 1
View(params)
etas <- c(0.05, 0.1, 0.15, 0.2)
nroundss <- c(50, 100, 200, 250)
max_depths <- c(2, 4, 6, 10)
colsample_bytrees <- c(0.5, 0.7, 0.8, 1)
colsample_bylevels <- c(0.33, 0.67, 1)
sub_samples <- c(0.25, 0.5, 0.75, 1)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
colsample_bylevel = colsample_bylevels,
sub_sample = sub_samples))
etas <- c(0.05, 0.1, 0.15, 0.2)
nroundss <- c(50, 100, 200, 250)
max_depths <- c(2, 4, 6, 10)
colsample_bytrees <- c(0.5, 0.7, 0.8, 1)
sub_samples <- c(0.25, 0.5, 0.75, 1)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
sub_sample = sub_samples))
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
this_sub_sample <- params[row, "sub_sample"]
model <- xgboost(
data = train_features,
label = train_labels,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
sub_sample = this_sub_sample,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_labels$strikeout_percentage))
if (mae < best_mae){
best_mae <- mae
best_param_row <- row
best_pred <- pred
best_model <- model
}
}
print(params[best_param_row, ])
subsamples <- c(0.25, 0.5, 0.75, 1)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
subsample = subsamples))
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
this_subsample <- params[row, "subsample"]
model <- xgboost(
data = train_features,
label = train_labels,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
subsample = this_subsample,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_labels$strikeout_percentage))
if (mae < best_mae){
best_mae <- mae
best_param_row <- row
best_pred <- pred
best_model <- model
}
}
print(params[best_param_row, ])
print(best_mae * length(pred))
print(0.0294 * length(pred))
caret::confusionMatrix(best_pred, test_labels$strikeout_percentage)
install.packages("caret")
install.packages("caret")
caret::confusionMatrix(best_pred, test_labels$strikeout_percentage)
library(DBI)
library(RPostgres)
library(RSQLite)
library(tidyverse)
library(xgboost)
prediction_vs_actual_df <- test_labels %>%
mutate(strikeout_percentage_pred = best_pred) %>%
mutate(absolute_error = abs(strikeout_percentage -
strikeout_percentage_pred)) %>%
arrange(absolute_error)
error_plot <- ggplot(prediction_vs_actual_df, aes(x=1:length(absolute_error),
y=absolute_error)) +
geom_line() +
theme(axis.title.x = element_blank())
importance_df <- as.data.frame(xgb.importance(colnames(df_train_features),
model = best_model)) %>%
select(Feature, Gain) %>%
arrange(Gain)
importance_df$Feature <- factor(importance_df$Feature,
levels = importance_df$Feature)
importance_plot <- ggplot(tail(importance_df, 10), aes(Gain, Feature)) +
geom_point() +
scale_x_continuous(breaks = seq(0, 0.2, 0.01)) +
labs(title="XGBoost Model Feature Importance (Top 10)")
View(prediction_vs_actual_df)
print(params[best_param_row, ])
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees)) %>%
mutate(mae = NA)
View(params)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
this_subsample <- params[row, "subsample"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
subsample = this_subsample,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- row
best_pred <- pred
best_model <- model
}
}
print(params[best_param_row, ])
df_train_features <- less_features_df %>%
subset(!(game_year %in% c(2019, 2021))) %>%
select(-pitcher, -name, -pitch_count, -games, -pitches_per_game, -p_throws,
-strikeout_percentage, -game_year)
df_train_label <- less_features_df %>%
subset(!(game_year %in% c(2019, 2021))) %>%
select(strikeout_percentage)
df_test_features <- less_features_df %>%
subset(game_year %in% c(2019, 2021)) %>%
select(-pitcher, -name, -pitch_count, -games, -pitches_per_game, -p_throws,
-strikeout_percentage, -game_year)
test_label <- less_features_df %>%
subset(game_year %in% c(2019, 2021)) %>%
select(name, p_throws, game_year, pitch_count, games, pitches_per_game,
strikeout_percentage)
train_features <- data.matrix(df_train_features)
train_label <- data.matrix(df_train_labels)
test_features <- data.matrix(df_test_features)
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
# this_subsample <- params[row, "subsample"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
# subsample = this_subsample,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- row
best_pred <- pred
best_model <- model
}
}
print(params[best_param_row, ])
View(params)
etas <- c(0.05, 0.1, 0.15, 0.2)
nroundss <- c(50, 100, 200, 250)
max_depths <- c(2, 4, 6)
colsample_bytrees <- c(0.5, 0.7, 0.8, 1)
subsamples <- c(0.25, 0.5, 0.75, 1)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees)) %>%
mutate(mae = NA)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
# this_subsample <- params[row, "subsample"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
# subsample = this_subsample,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- params[row, ]
best_pred <- pred
best_model <- model
}
}
print(best_params_row)
print(best_param_row)
gammas <- c(0.25, 0.5, 0.75, 1)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
gamma = gammas)) %>%
mutate(mae = NA)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
# this_subsample <- params[row, "subsample"]
this_gamma <- params[row, "gamma"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
# subsample = this_subsample,
gamma = this_gamma,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- params[row, ]
best_pred <- pred
best_model <- model
}
}
print(best_param_row)
etas <- c(0.05, 0.1, 0.15, 0.2)
nroundss <- c(50, 100, 200, 250)
max_depths <- c(2, 4, 6)
colsample_bytrees <- c(0.5, 0.7, 0.8, 1)
# subsamples <- c(0.25, 0.5, 0.75, 1)
gammas <- c(0, 1, 10)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
gamma = gammas)) %>%
mutate(mae = NA)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
# this_subsample <- params[row, "subsample"]
this_gamma <- params[row, "gamma"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
# subsample = this_subsample,
gamma = this_gamma,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- params[row, ]
best_pred <- pred
best_model <- model
}
}
print(best_param_row)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
gamma = gammas)) %>%
mutate(mae = NA)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
# this_subsample <- params[row, "subsample"]
this_gamma <- params[row, "gamma"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
# subsample = this_subsample,
gamma = this_gamma,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- params[row, ]
best_pred <- pred
best_model <- model
}
}
print(best_param_row)
min_child_weights <- c(0, 1, 10)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
min_child_weight = min_child_weights)) %>%
mutate(mae = NA)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
# this_subsample <- params[row, "subsample"]
this_min_child_weight <- params[row, "min_child_weight"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
# subsample = this_subsample,
min_child_weight = this_min_child_weight,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- params[row, ]
best_pred <- pred
best_model <- model
}
}
print(best_param_row)
mean(filter(params, colsample_by_tree == 0.7)$mae)
mean(filter(params, colsample_bytree == 0.7)$mae)
mean(filter(params, colsample_bytree == 0.8)$mae)
mean(filter(params, colsample_bytree == 0.5)$mae)
mean(filter(params, colsample_bytree == 1)$mae)
etas <- c(0.05, 0.1, 0.15, 0.2)
nroundss <- c(50, 100, 200, 250)
max_depths <- c(2, 4, 6)
colsample_bytrees <- c(0.5, 0.7, 0.8, 1)
subsamples <- c(0.1, 0.5, 0.9)
# min_child_weights <- c(0, 1, 10)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
# min_child_weight = min_child_weights,
subsample = subsamples)) %>%
mutate(mae = NA)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
this_subsample <- params[row, "subsample"]
# this_min_child_weight <- params[row, "min_child_weight"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
subsample = this_subsample,
# min_child_weight = this_min_child_weight,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- params[row, ]
best_pred <- pred
best_model <- model
}
}
print(best_param_row)
colsample_bytrees <- c(0.5, 0.7, 0.8, 1)
subsamples <- c(0.1, 0.5, 0.9)
lambdas <- c(0.1, 0.5, 0.9)
# min_child_weights <- c(0, 1, 10)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
# min_child_weight = min_child_weights,
# subsample = subsamples
lambda = lambdas)) %>%
mutate(mae = NA)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
# this_subsample <- params[row, "subsample"]
# this_min_child_weight <- params[row, "min_child_weight"]
this_lambda <- params[row, "lambda"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
# subsample = this_subsample,
# min_child_weight = this_min_child_weight,
lambda = this_lambda,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- params[row, ]
best_pred <- pred
best_model <- model
}
}
print(best_param_row)
max_depths <- c(2, 4, 6)
colsample_bytrees <- c(0.5, 0.7, 0.8, 1)
# subsamples <- c(0.1, 0.5, 0.9)
# min_child_weights <- c(0, 1, 10)
lambdas <- c(0.001, 0.1, 0.5, 0.9)
params <- expand.grid(list(eta = etas, nrounds = nroundss,
max_depth = max_depths,
colsample_bytree = colsample_bytrees,
# min_child_weight = min_child_weights,
# subsample = subsamples
lambda = lambdas)) %>%
mutate(mae = NA)
best_mae <- 1
for (row in 1:nrow(params)){
this_eta <- params[row, "eta"]
this_nrounds <- params[row, "nrounds"]
this_max_depth <- params[row, "max_depth"]
this_colsample_bytree <- params[row, "colsample_bytree"]
# this_subsample <- params[row, "subsample"]
# this_min_child_weight <- params[row, "min_child_weight"]
this_lambda <- params[row, "lambda"]
model <- xgboost(
data = train_features,
label = train_label,
eta = this_eta,
nrounds = this_nrounds,
max_depth = this_max_depth,
colsample_bytree = this_colsample_bytree,
# subsample = this_subsample,
# min_child_weight = this_min_child_weight,
lambda = this_lambda,
verbose = FALSE
)
pred <- predict(model, test_features)
mae <- mean(abs(pred - test_label$strikeout_percentage))
params[row, "mae"] <- mae
if (mae < best_mae){
best_mae <- mae
best_param_row <- params[row, ]
best_pred <- pred
best_model <- model
}
}
print(best_param_row)
